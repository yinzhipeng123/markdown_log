## 查看主题名为『__consumer_offsets』的分区数与副本数

```bash
[root@VM-0-16-centos bin]# sh kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic __consumer_offsets
Topic: __consumer_offsets       TopicId: 2G8TIlujSISTjGZ2NupZSQ PartitionCount: 50      ReplicationFactor: 1    Configs: compression.type=producer,cleanup.policy=compact,segment.bytes=104857600
        Topic: __consumer_offsets       Partition: 0    Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 1    Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 2    Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 3    Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 4    Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 5    Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 6    Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 7    Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 8    Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 9    Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 10   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 11   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 12   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 13   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 14   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 15   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 16   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 17   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 18   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 19   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 20   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 21   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 22   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 23   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 24   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 25   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 26   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 27   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 28   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 29   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 30   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 31   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 32   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 33   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 34   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 35   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 36   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 37   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 38   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 39   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 40   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 41   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 42   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 43   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 44   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 45   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 46   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 47   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 48   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
        Topic: __consumer_offsets       Partition: 49   Leader: 1       Replicas: 1     Isr: 1  Elr: N/A        LastKnownElr: N/A
```



`kafka-topics.sh --describe` 命令时，输出提供了关于指定 topic 的详细信息。每个字段都对应着 Kafka 内部的不同配置和状态。以下是你输出的字段解释：

1. **Topic: __consumer_offsets**

这是 topic 的名称，表示当前查询的是 `__consumer_offsets` 这个 topic。

2. **TopicId: 2G8TIlujSISTjGZ2NupZSQ**

这是该 topic 的唯一标识符，用于 Kafka 内部区分不同的 topic。

3. **PartitionCount: 50**

表示该 topic 有多少个分区。在你的 case 中，`__consumer_offsets` 共有 50 个分区。

4. **ReplicationFactor: 1**

这是该 topic 的副本因子，表示每个分区的副本数。在你的 case 中，副本因子为 1，意味着每个分区只有一个副本，只有一个 Leader。

5. **Configs: compression.type=producer,cleanup.policy=compact,segment.bytes=104857600**

这些是 topic 的配置信息：

- [ ] **compression.type=producer**：表示生产者发送消息时采用的压缩类型。此处是 `producer`，即由生产者决定压缩类型。
- [ ] **cleanup.policy=compact**：表示该 topic 的清理策略为 "压缩"，意味着 Kafka 会保留最新的消息，删除已消费的消息，以减少存储。
- [ ] **segment.bytes=104857600**：表示每个 segment 文件的大小为 100MB。

接下来是关于每个分区的详细信息：

6. **Topic: __consumer_offsets Partition: 0**

表示这是 `__consumer_offsets` topic 的第 0 个分区。

7. **Leader: 1**

表示这个分区的 Leader 所在的 broker ID。Leader 是负责处理客户端请求（如生产和消费）的分区副本，所有操作都通过 Leader 进行。

8. **Replicas: 1**

表示该分区的副本列表，当前是 `1`，表示该分区只有一个副本，也就是 Leader 本身没有其他副本。

9. **Isr: 1**

表示该分区的 ISR（In-Sync Replicas，表示当前同步的副本）。由于副本因子为 1，所以 ISR 只有一个副本，即 Leader 自身。通常，ISR 包含所有与 Leader 同步的副本。

10. **Elr: N/A**

这个字段表示 "Estimated Log Retention"（估计的日志保留），指示当前的分区数据是否会因为保留策略而被清理。由于是 `__consumer_offsets` 这种特殊的系统 topic，通常日志会按策略进行压缩，因此此字段为 N/A。

11. **LastKnownElr: N/A**

表示上次已知的 ELR（Estimated Log Retention）。如果没有日志保留策略或此 topic 配置不同，也可能是 N/A。







## 设置主题的分区数与副本数

设置分区数和副本数

- [ ] **分区数（Partitions）**：决定了该主题的数据分布。每个分区是消息存储的基本单元，Kafka 通过分区实现数据的并行处理。
- [ ] **副本数（Replicas）**：决定了该主题的副本数，确保数据的冗余和高可用性。

在创建主题时设置分区数和副本数

在 Kafka 中， Kafka 的命令行工具来创建主题并指定分区数和副本数。以下是命令格式：

```bash
kafka-topics.sh --create --topic <topic_name> --partitions <num_partitions> --replication-factor <num_replicas> --bootstrap-server <kafka_broker>
```

- [ ] `<topic_name>`：指定主题的名称。
- [ ] `<num_partitions>`：指定分区数。
- [ ] `<num_replicas>`：指定副本数。
- [ ] `<kafka_broker>`：Kafka 集群中的任意一个 broker 地址。

示例：

想要创建一个名为 `my_topic` 的主题，分区数为 3，副本数为 2，Kafka 的 Broker 地址为 `localhost:9092`，可以运行以下命令：

```bash
kafka-topics.sh --create --topic my_topic --partitions 3 --replication-factor 2 --bootstrap-server localhost:9092
```

配置示例说明：

- [ ] `--partitions 3`：表示该主题会有 3 个分区。消息会被均匀地分布到这 3 个分区中。
- [ ] `--replication-factor 2`：表示该主题的副本数为 2，每个分区会有一个主副本和一个从副本。

查看已创建的主题信息

如果你想查看已创建主题的分区数和副本数，可以使用以下命令：

```bash
kafka-topics.sh --describe --topic <topic_name> --bootstrap-server <kafka_broker>
```

例如，要查看 `my_topic` 的分区和副本信息：

```bash
kafka-topics.sh --describe --topic my_topic --bootstrap-server localhost:9092
```

该命令将显示有关主题的详细信息，包括分区和副本的分布情况。

注意事项：

- **分区数**：一旦主题创建后，分区数是不可更改的。如果需要增加分区数，可以使用 `kafka-topics.sh` 的 `--alter` 命令来修改分区数，但这会影响分区的负载和消费者的处理方式，因此要小心操作。
- **副本数**：副本数也可以通过 `--alter` 命令修改，但同样需要谨慎操作，特别是在副本调整过程中可能会出现的网络延迟和负载波动。

生产者发送消息时的分区选择

当你发送消息到 Kafka 时，生产者通常会自动选择一个分区。如果你没有指定分区，生产者会使用 **分区器**（Partitioner）根据消息的 **key** 来决定消息发送到哪个分区。例如，`key` 值相同的消息将会被发送到同一个分区，这样可以保证它们的顺序性。

如果你想手动指定消息的分区，可以通过设置 `produce()` 方法中的 `partition` 参数来指定目标分区。

```python
producer.produce('my_topic', key='key1', value='value1', partition=1)  # 指定发送到分区1
```