



在推理阶段，大模型是 **逐字生成** 的。
 假设我们要让模型输出一句话，比如：

> “今天天气真好”

模型会这样一步步生成：

1. 输入 “今” → 输出 “天”
2. 输入 “今天” → 输出 “天”
3. 输入 “今天天” → 输出 “气”
4. 以此类推……

每次都要重新计算前面所有 token 的 Q、K、V，这样会非常慢！
 → 因此，引入了 **KV cache（键值缓存）**。



KV 是 **Key** 和 **Value** 的缩写。
 在 Transformer 的 **注意力机制（Attention）** 中，每个输入 token 都会被映射成三个向量：

- **Q（Query）**
- **K（Key）**
- **V（Value）**



vLLM 对 KV cache 做了很厉害的优化，它使用一种叫做 **PagedAttention** 的机制（“分页注意力”），让不同请求可以共享和复用 KV cache 的显存页，大大提升显存利用率。