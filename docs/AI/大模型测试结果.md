



```bash
python3 benchmark_serving.py --help
usage: benchmark_serving.py [-h] [--backend {tgi,vllm,lmdeploy,deepspeed-mii,openai,openai-chat,tensorrt-llm,scalellm,sglang}]
                            [--base-url BASE_URL] [--host HOST] [--port PORT] [--endpoint ENDPOINT]
                            [--dataset-name {sharegpt,burstgpt,sonnet,random,hf}] [--dataset-path DATASET_PATH]
                            [--max-concurrency MAX_CONCURRENCY] --model MODEL [--tokenizer TOKENIZER] [--best-of BEST_OF]
                            [--use-beam-search] [--num-prompts NUM_PROMPTS] [--logprobs LOGPROBS] [--request-rate REQUEST_RATE]
                            [--burstiness BURSTINESS] [--seed SEED] [--trust-remote-code] [--disable-tqdm] [--profile] [--save-result]
                            [--metadata [KEY=VALUE ...]] [--result-dir RESULT_DIR] [--result-filename RESULT_FILENAME] [--ignore-eos]
                            [--percentile-metrics PERCENTILE_METRICS] [--metric-percentiles METRIC_PERCENTILES]
                            [--goodput GOODPUT [GOODPUT ...]] [--sonnet-input-len SONNET_INPUT_LEN]
                            [--sonnet-output-len SONNET_OUTPUT_LEN] [--sonnet-prefix-len SONNET_PREFIX_LEN]
                            [--sharegpt-output-len SHAREGPT_OUTPUT_LEN] [--random-input-len RANDOM_INPUT_LEN]
                            [--random-output-len RANDOM_OUTPUT_LEN] [--random-range-ratio RANDOM_RANGE_RATIO]
                            [--random-prefix-len RANDOM_PREFIX_LEN] [--hf-subset HF_SUBSET] [--hf-split HF_SPLIT]
                            [--hf-output-len HF_OUTPUT_LEN] [--tokenizer-mode {auto,slow,mistral,custom}]
                            [--served-model-name SERVED_MODEL_NAME] [--lora-modules LORA_MODULES [LORA_MODULES ...]]

# Benchmark the online serving throughput.   # 这是说明，脚本是用来评测模型在线推理吞吐量的。

options:
  -h, --help            show this help message and exit   # 显示帮助信息并退出
  --backend {tgi,vllm,lmdeploy,deepspeed-mii,openai,openai-chat,tensorrt-llm,scalellm,sglang}  
                        # 指定后端框架或服务类型，例如 vllm、tgi 等
  --base-url BASE_URL   Server or API base url if not using http host and port.  # 如果不指定 host/port，用这个 URL
  --host HOST           # 服务 host
  --port PORT           # 服务端口
  --endpoint ENDPOINT   API endpoint.  # API 接口路径
  --dataset-name {sharegpt,burstgpt,sonnet,random,hf}  
                        # 要用来 benchmark 的数据集类型
  --dataset-path DATASET_PATH
                        # 数据集路径，或者 HuggingFace 数据集 ID
  --max-concurrency MAX_CONCURRENCY
                        # 最大并发请求数，用于限制同时发送的请求数量
  --model MODEL         Name of the model.  # 模型名称或路径
  --tokenizer TOKENIZER
                        # 指定分词器，默认用模型自带
  --best-of BEST_OF     Generates `best_of` sequences per prompt and returns the best one.  # 每个 prompt 生成多条序列，取最佳
  --use-beam-search     # 使用 beam search 解码
  --num-prompts NUM_PROMPTS
                        # 要处理的 prompt 数量
  --logprobs LOGPROBS   Number of logprobs-per-token to compute & return as part of the request.  
                        # 每个 token 返回的 logprob 数量
  --request-rate REQUEST_RATE
                        # 每秒发送请求数
  --burstiness BURSTINESS
                        # 请求突发性系数，控制请求时间间隔分布
  --seed SEED           # 随机种子
  --trust-remote-code   Trust remote code from huggingface  # 信任 HuggingFace 远程代码
  --disable-tqdm        Specify to disable tqdm progress bar.  # 禁用进度条
  --profile             Use Torch Profiler. The endpoint must be launched with VLLM_TORCH_PROFILER_DIR to enable profiler.  
                        # 使用 Torch Profiler 分析性能
  --save-result         Specify to save benchmark results to a json file  # 保存结果到 JSON
  --metadata [KEY=VALUE ...]
                        # 为本次 benchmark 添加元信息
  --result-dir RESULT_DIR
                        # JSON 保存目录
  --result-filename RESULT_FILENAME
                        # JSON 文件名
  --ignore-eos          Set ignore_eos flag when sending the benchmark request. Warning: ignore_eos is not supported in deepspeed_mii and tgi.  
                        # 忽略 EOS token
  --percentile-metrics PERCENTILE_METRICS
                        # 指定要计算百分位数的指标，例如 ttft、tpot、itl、e2el
  --metric-percentiles METRIC_PERCENTILES
                        # 指定百分位数列表，例如 "25,50,75"
  --goodput GOODPUT [GOODPUT ...]
                        # 服务级别指标设置，如 ttft:1000（单位 ms）
  --tokenizer-mode {auto,slow,mistral,custom}
                        # 分词器模式：auto/slow/mistral/custom
  --served-model-name SERVED_MODEL_NAME
                        # API 中使用的模型名称
  --lora-modules LORA_MODULES [LORA_MODULES ...]
                        # 指定 LoRA 模块子集，每次请求随机选择一个

# sonnet dataset options:
  --sonnet-input-len SONNET_INPUT_LEN
                        # 每条请求输入 token 数
  --sonnet-output-len SONNET_OUTPUT_LEN
                        # 每条请求输出 token 数
  --sonnet-prefix-len SONNET_PREFIX_LEN
                        # 每条请求前缀 token 数

# sharegpt dataset options:
  --sharegpt-output-len SHAREGPT_OUTPUT_LEN
                        # 每条请求输出长度，覆盖 ShareGPT 数据集自带的长度

# random dataset options:
  --random-input-len RANDOM_INPUT_LEN
                        # 随机生成输入 token 数
  --random-output-len RANDOM_OUTPUT_LEN
                        # 随机生成输出 token 数
  --random-range-ratio RANDOM_RANGE_RATIO
                        # 输入/输出长度采样比例范围
  --random-prefix-len RANDOM_PREFIX_LEN
                        # 随机上下文前缀长度，最终长度 = prefix_len + prefix_len*range_ratio

# hf dataset options:
  --hf-subset HF_SUBSET
                        # HuggingFace 数据集子集
  --hf-split HF_SPLIT   # 数据集拆分（train/test/validation）
  --hf-output-len HF_OUTPUT_LEN
                        # 输出 token 数，覆盖数据集自带长度
```



```bash
python3 benchmark_serving.py \                         # 使用 python3 运行 vLLM 的服务性能测试脚本
        --host 0.0.0.0 \                               # 服务监听地址，0.0.0.0 表示对所有网卡开放（允许外部访问）
        --port 8802 \                                  # HTTP API 服务端口
        --backend vllm \                               # 使用 vLLM 作为推理后端（高性能大模型推理引擎）
        --model /workspace/DeepSeek-R1-0528-tokenizer \ # 加载的模型路径（本地 DeepSeek-R1 tokenizer + model 目录）
        --dataset-name random \                        # 使用随机生成的数据作为测试集（用于压力/性能测试）
        --max-concurrency 10\                          # 最大并发请求数：同时最多允许 10 个请求压测
        --num-prompts 50 \                             # 总共发送 50 个请求（prompt）
        --random-input-len 1024 \                      # 每个请求输入长度为 1024 tokens（模拟长提示词）
        --random-output-len 1024 \                     # 每个请求生成 1024 tokens（模拟长文本生成）
        --ignore-eos                                   # 忽略 EOS（End of Sequence），强制模型生成到指定长度
```





```bash
============ Serving Benchmark Result ============  
Successful requests:                     100        # 成功返回结果的请求数（共发送了100个请求，全都成功执行）  
Benchmark duration (s):                  21.06      # 整个基准测试持续时间，共耗时21.06秒  
Total input tokens:                      51200      # 所有请求的输入token总数（100条 × 每条输入512个token）  
Total generated tokens:                  51200      # 模型生成的输出token总数（100条 × 每条输出512个token）  
Request throughput (req/s):              4.75       # 每秒平均可处理的请求数（Request Per Second，越高代表吞吐量越好）  
Output token throughput (tok/s):         2430.79    # 每秒输出的token数（仅计算生成部分，体现模型生成速度）  
Total Token throughput (tok/s):          4861.57    # 每秒处理的总token数（输入+输出），衡量整体吞吐能力  
---------------Time to First Token----------------  
Mean TTFT (ms):                          3719.54    # 平均“首个token响应时间”（从发送请求到模型输出第一个token的平均延迟，越低越好）  
Median TTFT (ms):                        3792.08    # 中位数TTFT（排除极端值后中间水平的首token延迟）  
P99 TTFT (ms):                           4955.84    # 第99百分位TTFT（最慢1%请求的首token延迟，表示极端情况下响应时间） 
-----Time per Output Token (excl. 1st token)------  
Mean TPOT (ms):                          33.88      # 平均每个输出token生成耗时（不含首token），体现模型连续生成速度  
Median TPOT (ms):                        33.74      # 中位数TPOT，剔除异常延迟后更稳态的生成速度  
P99 TPOT (ms):                           39.43      # 第99百分位TPOT（最慢1%的token生成耗时）  
---------------Inter-token Latency----------------  
Mean ITL (ms):                           34.42      # 平均token间延迟（首token后每个token之间的时间间隔，越低生成越流畅）  
Median ITL (ms):                         31.91      # 中位数token间延迟  
P99 ITL (ms):                            79.97      # 第99百分位token间延迟，说明在极端情况下，token间最大延迟可达约80ms 
==================================================
```



