



### 1️⃣ 常见浮点数类型

| 类型 | 精度位数 | 字节数 | 说明                                  |
| ---- | -------- | ------ | ------------------------------------- |
| FP32 | 32 位    | 4 字节 | 单精度浮点数，原始训练常用            |
| FP16 | 16 位    | 2 字节 | 半精度浮点数，推理/训练常用           |
| BF16 | 16 位    | 2 字节 | 类似 FP16，但指数范围更大，更适合训练 |
| INT8 | 8 位     | 1 字节 | 量化推理用，精度下降但显存占用少      |

一些模型会用 **“B”** 表示十亿参数（B = billion），所以看到 `7B` 就可以估算显存。

例如：

- 模型：Qwen-7B
- 参数量：7B = 7,000,000,000 个参数
- 数据类型：FP16（每个参数 2 字节 = 16 bit）



7,000,000,000×2 字节=14,000,000,000 字节

（1 GB = 1024³ ≈ 1,073,741,824 字节）

显存=14,000,000,000/1,073,741,824≈13.04 GB





实际推理时还需要考虑以下几个额外开销：

- **激活（activations）／中间层输出**：模型前向传播时，会为每层计算输出，保存暂存数据、缓存、以及用于 attention 的中间值。
- **KV cache 或 past key/value 缓存**：在生成模型里，为了上下文延续，会缓存 key/value 对，这也会占显存。
- **序列长度 (context length) 和 batch size 的影响**：如果输入序列特别长，或者 batch size 大，这部分显存开销会显著增加。
- **显存对齐、库 overhead、显存碎片**：框架（如 vLLM、Transformers）还可能预分配或保留部分显存以便效能优化。
- **精度和量化**：如果不是纯 FP16，而是用 FP32、或者用量化（INT8/INT4）或有特殊缓存方式，显存占用会变。
- **并行／设备映射 (device_map)、显存切片 (sharding)、外部 off-load** 等也会影响实际占用结构。