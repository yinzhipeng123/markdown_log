### 1. Transformer 是什么

Transformer 是一种 **神经网络架构**，主要用来处理 **序列数据**（比如文本、语音、时间序列等）。它在 2017 年被提出，最初用于 **机器翻译**（比如把英文翻译成中文），后来成为自然语言处理（NLP）的主力架构，也扩展到图像、音频等领域。

它的核心创新是 **“自注意力机制（Self-Attention）”**，让模型能 **同时看到整个序列的信息**，而不是像 RNN 一样一个时间步一个时间步地处理。

------

### 2. Transformer 的特点

1. **并行计算能力强**：不像 RNN 需要顺序计算，Transformer 可以同时处理序列中的所有元素。
2. **捕捉长距离依赖**：通过自注意力，序列中任意两个位置都能直接交互信息。
3. **结构模块化**：主要由 **Encoder**（编码器）和 **Decoder**（解码器）组成，每个模块里都有多层 **自注意力 + 前馈网络**。

------

### 3. 数学直观例子（很简化）

假设有一句话：`["我", "爱", "学习"]`，Transformer 通过自注意力计算每个词对其他词的重要性：
$$
\text{Attention}(\text{词}_i) = \sum_j \text{权重}_{ij} \cdot \text{词向量}_j
$$
这里：

- $\text{词向量}_j$ 是每个词的向量表示
- $\text{权重}_{ij}$ 表示词 $i$ 对词 $j$ 的关注程度

这样，“学习”这个词可以直接参考“我”和“爱”的信息，不必按顺序慢慢传播。

------

### 4. Transformer 的应用

- **NLP**：翻译、问答、文本生成（GPT、BERT 等都是 Transformer 系列）
- **图像**：Vision Transformer (ViT)
- **音频**：语音识别、音乐生成