大模型启动

```bash
unset XPU_DUMMY_EVENT                   # 移除虚拟事件占位符，确保系统使用真实的 XPU 事件处理机制
export XPU_VISIBLE_DEVICES=0,1,2,3,4,5,6,7  # 指定当前程序可以使用的 Intel XPU 物理卡编号
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 # 兼容性设置，让识别 CUDA 的库也能看到对应的 XPU 设备
export XFT_USE_FAST_SWIGLU=1            # 启用 Fast SwiGLU 激活函数优化，提升计算效率
export XMLIR_CUDNN_ENABLED=1            # 开启 XMLIR 中模拟 cuDNN 的加速特性
export XPU_USE_DEFAULT_CTX=1            # 强制使用默认的 XPU 上下文，减少多卡环境下的资源冲突
export XMLIR_FORCE_USE_XPU_GRAPH=1      # 强制使用 XPU 图执行模式，类似 CUDA Graph，减少 CPU 调度开销
export BKCL_FORCE_SYNC=1                # 强制集合通信（BKCL）同步执行，提升分布式环境下的稳定性
export XPU_USE_MOE_SORTED_THRES=120     # 设置 MoE（混合专家模型）排序阈值，用于平衡负载与计算性能
export XPU_USE_FAST_SWIGLU=1            # 重复确认启用 Fast SwiGLU（同上，确保生效）
export XMLIR_ENABLE_MOCK_TORCH_COMPILE=false # 禁用模拟的 Torch 编译，通常为了避免与 vLLM 内部编译冲突
export USE_ORI_ROPE=1                   # 使用原始的旋转位置编码（RoPE）计算逻辑，不使用变体
export VLLM_USE_V1=1                    # 强制 vLLM 使用 V1 版本的执行引擎（通常用于兼容特定硬件后端）

python -m vllm.entrypoints.openai.api_server \
    --host localhost \                  # 服务绑定地址为本地回环
    --port 8080 \                       # 监听端口号为 8080
    --model /data/ssd3/models/Qwen3-32B \ # 模型权重文件的物理存放路径
    --gpu-memory-utilization 0.95 \     # 显存占用比例设置，此处允许使用 95% 的可用显存
    --trust-remote-code \               # 允许执行模型仓库中的自定义 Python 代码
    --max-model-len 32768 \             # 设置模型支持的最大序列长度
    --tensor-parallel-size 2 \          # 张量并行度，即将模型拆分并在 2 张卡上协同运行
    --dtype float16 \                   # 推理所使用的数值精度类型（半精度浮点）
    --max_num_seqs 400 \                # 单次并发处理的最大序列数（Batch Size 相关）
    --max_num_batched_tokens 32768 \    # 单次 Batch 能够处理的最大 Token 总数
    --max-seq-len-to-capture 32768 \    # 图捕获模式（Graph Capture）支持的最大长度
    --block-size 128 \                  # PagedAttention 的内存块大小，设为 128 提升吞吐
    --distributed-executor-backend mp \ # 分布式执行器后端使用 multiprocessing (多进程) 模式
    --no-enable-prefix-caching \        # 禁用前缀缓存功能（通常在显存极度紧张时关闭）
    --no-enable-chunked-prefill \       # 禁用分块预填充，一次性处理完整 Prompt（适合长文本）
    --compilation-config '{"splitting_ops": ["vllm.unified_attention_with_output_kunlun","vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"]}' # 针对昆仑芯/XPU 优化的算子编译拆分配置
```





```bash
max-model-len	单个用户最多 32K
max_num_batched_tokens	一次调度最多 32K
max-seq-len-to-capture	为 32K 长度生成 CUDA Graph
```

