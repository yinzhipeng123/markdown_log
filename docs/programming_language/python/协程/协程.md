# 协程

### 什么是协程

协程是轻量级线程，拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来时，恢复先前保存的寄存器上下文和栈。因此，协程能保留上一次调用时的状态，即所有局部状态的一个特定组合，每次过程重入时，就相当于进入上一次调用的状态。

### 什么时候用

协程的应用场景：I/O密集型任务。这一点与多线程有些类似，但协程调用是在一个线程内进行的，是单线程，切换的开销小，因此效率上略高于多线程。当程序在执行IO操作时，CPU是空闲的，可以充分利用CPU的时间片来处理其他任务。在单线程中，一个函数调用，一般是从函数的第一行代码开始执行，结束于return语句、异常或函数执行（也可以认为是隐式地返回了None）。有了协程，我们在函数的执行过程中，如果遇到了耗时的I/O操作，函数就可以临时让出控制权，让CPU执行其他函数，等I/O操作执行完毕后再收回控制权。

#### 定义协程

首先引入asyncio，这样才可以使用async和await关键字（async定义一个协程，await用于临时挂起一个函数或方法的执行），接着使用async定义一协程方法，然后直接调用该方法，但是该方法并没有执行，而是返回了一个coroutine协程对象。使用get_event_loop()方法创建一个事件循环loop，并调用loop对象的run_until_complete()方法将协程注册到事件循环loop中，然后启动，最后才看到task方法打印了输出结果。async定义的方法无法直接执行，必须将其注册到事件循环中才可以执行。

```python
import asyncio
import time


async def task():
    print(f"{time.strftime('%H:%M:%S')}task开始")
    time.sleep(2)
    print(f"{time.strftime('%H:%M:%S')}task结束")


coroutine = task()
print(f"{time.strftime('%H:%M:%S')} 产生协程对象 {coroutine},函数未被调用")
loop = asyncio.get_event_loop()
print(f"{time.strftime('%H:%M:%S')} 开始调用协程")
start = time.time()
loop.run_until_complete(coroutine)
end = time.time()
print(f"{time.strftime('%H:%M:%S')} 结束调用协程，耗时{end - start} 秒")

```

运行结果：

```bash
21:14:47 产生协程对象 <coroutine object task at 0x7f98000ddbc0>,函数未被调用
21:14:47 开始调用协程
21:14:47task开始
21:14:49task结束
21:14:49 结束调用协程，耗时2.0022308826446533 秒
```

我们还可以为任务绑定回调函数。

在这里我们定义了一个协程方法和一个普通方法作为回调函数，协程方法执行后返回一个字符串'运行束'。其中回调函数接收一个参数，是task对象，然后调用print()方法打印了task对象的结果。asyncio.ensure_future(coroutine)可以返回task对象，add_done_callback()为task对象增加一个回调任务。这样我们就定义好了一个coroutine对象和一个回调方法，执行的结果是当coroutine对象执行完毕之后，就去执行声明的callback()方法。

```python
import asyncio
import time


async def _task():
    print(f"{time.strftime('%H:%M:%S')} task开始")
    time.sleep(2)
    print(f"{time.strftime('%H:%M:%S')} task结束")
    return "运行结束"


def callback(task):
    print(f"{time.strftime('%H:%M:%S')} 开始调用协程")
    print(f"状态：{task.result()}")


coroutine = _task()
print(f"{time.strftime('%H:%M:%S')} 产生协程对象 {coroutine},函数未被调用")
task = asyncio.ensure_future(coroutine)
task.add_done_callback(callback)
loop = asyncio.get_event_loop()
print(f"{time.strftime('%H:%M:%S')} 开始调用协程")
start = time.time()
loop.run_until_complete(task)
end = time.time()
print(f"{time.strftime('%H:%M:%S')} 结束调用协程，耗时{end - start} 秒")
```

运行结果：

```python
21:26:21 产生协程对象 <coroutine object _task at 0x7fdb8479eac0>,函数未被调用
21:26:21 开始调用协程
21:26:21 task开始
21:26:23 task结束
21:26:23 开始调用协程
状态：运行结束
21:26:23 结束调用协程，耗时2.0030200481414795 秒
```



#### 并发

我们只执行了一个协程任务，如果需要执行多次并尽可能地提高效率该怎么办呢？我们可以定义一个task列表，然后使用asyncio的wait()方法执行即可

await后面的对象必须是如下类型之一。

- 一个原生coroutine对象。
- 一个由types.coroutine()修饰的生成器，这个生成器可以返回coroutine对象。
- 一个包含await方法的对象返回的一个迭代器。

asyncio.sleep（2）是一个由coroutine修饰的生成器函数，表示等待2秒。接下来我们定义了一个列表tasks，由5个task()组成，最后使用loop.run_until_complete(asyncio.wait(tasks))提交执行，即5个任务并发执行，耗时接近于单个任务的耗时，这里并没有使用多进程或多线程，从而实现了并发操作。task可以替换为任意耗时较高的I/O操作函数。

```python
import asyncio
import time


async def task():
    print(f"{time.strftime('%H:%M:%S')}task开始")
    #模拟任务
    await asyncio.sleep(2)
    print(f"{time.strftime('%H:%M:%S')}task结束")


# 获取 EventLoop:
loop = asyncio.get_event_loop()
# 执行 coroutine
tasks = [task() for _ in range(5)]
start = time.time()
loop.run_until_complete(asyncio.wait(tasks))
loop.close()
end = time.time()
print(f"用时{end - start}秒")

```



#### 实用样例：

我们以常用的网络请求场景为例，网络请求较多的应用就是I/O密集型任务。首先需要建立一个服务器来响应Web请求，为方便演示，我们使用轻量级的Web框架Flask来建立一个服务器。

```Python
from flask import Flask
import time

app = Flask(__name__)

@app.route('/')
def index():
    time.sleep(3)
    return  'hello world'

if __name__=='__main__':
    app.run(threaded=True)
```

在上述代码中，我们定义了一个Flask服务，主入口是index()方法，方法中先调用了sleep()方法休眠3秒，然后返回结果。也就是说，每次请求这个接口至少要耗时3秒，这样我们就模拟了一个慢速的服务接口。注意，服务启动时，run()方法添加了一个参数threaded，表明Flask启动了多线程模式，否则默认是只有一个线程的。如果不开启多线程模式，那么同一时刻遇到多个请求时，只能顺次处理，这样即使我们使用协程异步请求了这个服务，也只能一个个排队等待，瓶颈就会出现在服务端。所以，多线程模式是有必要打开的。

协程去请求

要实现异步，我们可以使用await将耗时等待的操作挂起让出控制权。当协程执行时遇到await，时间循环就会将本协程挂起，转而去执行别的协程，直到其他的协程挂起或执行完毕。

aiohttp是一个支持异步请求的库，利用它和anyncio配合，即可实现异步请求操作。

```python
import asyncio
import aiohttp
import time

now = lambda: time.strftime("%H:%M:%S")


async def get(url):
    session = aiohttp.ClientSession()
    response = await session.get(url)
    result = await response.text()
    await session.close()
    return result


async def request():
    url = 'http://127.0.0.1:5000'
    print(f"{now()} 请求 {url}")
    result = await get(url)
    print(f"{now()} 得到响应 {result}")


start = time.time()
####多并发
tasks = [asyncio.ensure_future(request()) for _ in range(50)]
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
###
end = time.time()
print(f"耗时 {end - start} 秒")

```



运行结果符合异常请求，耗时由15秒变成了3秒，即原来的1/5，实现了并发访问。代码中我们使用了await，后面跟了get()方法，在执行这5个协程的时候，如果遇到await，就会将当前协程挂起，转而去执行其他的协程，直到其他的协程也挂起或执行完毕，再进行下一个协程的执行。异步操作的便捷之处是，当遇到阻塞式操作时，任务被挂起，程序接着去执行其他的任务，而不是傻傻地等着，这样就可以充分利用CPU时间，而不必把时间浪费在等待I/O上。在发出网络请求后的3秒内，CPU都是空闲的，那么增加协程任务的数量，最终的耗时还会是3秒吗？理论来说确实是这样的，不过有一个前提，那就是服务器在同一时刻接受无限次请求都能保证正常返回结果，也就是服务器无限抗压，另外还要忽略I/O传输时延。我们可以将上述的任务数扩大20倍，如下：

```python
tasks = [asyncio.ensure_future(request()) for _ in range(20)]
```

最终的耗时如下：

```bash
耗时 3.0600500106811523 秒
```

运行时间也是在3秒左右，当然多出来的时间就是I/O时延了。可见，使用了异步协程之后，几乎可以在相同的时间内实现成百上千倍次的网络请求，把这个技术运用在爬虫项目中，速度提升可谓是非常可观了。